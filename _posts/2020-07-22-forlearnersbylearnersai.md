---
layout: post
title: "How Multi-Layer Neural Networks Work - Htun Khaing Lynn  မှ ရေးသားသည်။"
tag: ["လေ့လာနေသူများမှ လေ့လာမည့်သူများဆီသို့ - Artificial Intelligence"]
---
<br >
ကိုကျော်အလုပ်မှ အိမ်ပြန်ရောက်လာသည် ရေမိုးချိုးစားသောက်အပြီး ဇော်ဇော်မှ အားကြိုးမာန်တက်ဖြင့် 

"ကိုကျော်ရေ ဟိုနေ့ကဟာလေးဆက်ပါဦး ဟီးဟီး"

"ဟုတ်ပြီ...ကဲမနေ့ကျန်ခဲ့တာလေး ဆက်ရအောင်။ မနေ့ကပြောခဲ့တဲ့ထဲမှာ ကိုကျော်တို့က OR Gate ကိုSingle Layer Preceptronနဲ့ ဘယ်လိုလုပ်ရသဲသိပြီဟုတ်။ ဒါပေမယ့် Single layer Preceptron က ကိုကျော်တို့ OR တို့ AND တို့လို Linear function တွေကိုပဲလုပ်ဆောင်နိုင်တယ်"
<!-- more -->

<img src="http://drive.google.com/uc?export=view&id=1sO50xEWZ1yyzr1vkuCLwNIJgtOvyFhfS" alt="OR Gate Function">
*OR Gate Function*

<img src="http://drive.google.com/uc?export=view&id=1E607s5TgIRdfOd_MdgdpvkHAjWWr8j9K" alt="AND Gate Function">
*AND Gate Function*

"ပုံမှာပြထားတဲ့ အတိုင်းဆို OR တို့ AND 
တို့ကိုမျဥ်းတစ်ကြောင်းထဲနဲ့ဖြတ်ပြလိုက်လို့ရတယ်။ 

ဒါနဲ့ မေးရဦးမယ် တစ်ကြောင်းထဲနဲ့ ဖြတ်ပြလို့ရရင် ဘယ်လို ကိစ္စမျိုးအတွက် အဆင်ပြေမလဲ ဇော်ဇော်
အာ...အင်း..သိပြီ

(၁) Linear အတိုင်းရှိမယ့် point တွေကို တွက်ထုတ်ပေးနိုင်သလို<br >
(၂) Line တစ်ကြောင်းတည်းနဲ့ Classify လုပ်လို့ရတဲ့ကိစ္စတွေအတွက်လည်း အဆင်ပြေမယ်ထင်တယ်

အာ...ငါ့ညီ က ဘယ်ဆိုးလို့လဲဟ...အေး..ဒါပေမယ့် Non linear functions တွေဖြစ်တဲ XOR လိုမျိုးကိုကျ မျဥ်းတစ်ကြောင်းထဲနဲ့ဘယ်လိုမှဖြတ်အောင်လုပ်လို့မရဘူး ပုံကိုကြည့်လိုက်"

<img src="http://drive.google.com/uc?export=view&id=1A37kuv-TWdcuqc-m4rKag7Hzk9pspJOR" alt="XOR Gate Function">
*XOR Gate Function*

ဇော်ဇော် အနည်းငယ်တွေဝေသွားသည်။

"အဲ...ဟုတ်သားပဲ။ ဒါဆိုဘယ်လိုလုပ်ရမလဲကိုကျော်။ အလင်းပြပါဦး''

"အိုခေ။ အဲ့တာဆိုစကြမယ်။ Multi Layer Neural Networks မှာ အဲ့လို operationတွေကိုလုပ်ဆောင်နိုင်ဖို့အတွက် Back propagationဆိုတဲ့ Algorithm ပါလာတယ် ဒါပေမယ့် ဒါကိုအကျယ်မပြောသေးပါဘူး အကျဥ်းချုံ့ပဲပြောပြမယ်။ နောက်နေ့တွေမှအကျယ်ရှင်းကြတာပေါ့။ ဒါဆို XOR functionကိုကြည့်ရအောင်။ XOR ကဘယ်လိုမျိုးလဲ မှတ်မိသေးလားငါ့ညီ။

"အဲ...တူရင် 0 မတူရင် 1 ဟုတ်လား ကိုကျော်''

"အေးကွ...ဟုတ်တယ်ဒါဆို

| Input| |Output | 
|x1|x2| y|
|0 | 0 |0|
|0 | 1 |1|
|1 | 0 |1|
|1 | 1 |1| 

ဒါမျိုးရမယ်ပေါ့ ဒီအခါ ဟိုနေ့က Lecture ပြောခဲ့တဲ့အဆင့်တွေအတိုင်းပဲ input တွေ ကို Neuron ထဲပို့ရမယ် ဒါပေမယ့် ဝင်ရမယ့် Neuron က နှစ်လုံးဖြစ်သွားမယ်ပေါ့ ဒီပုံကိုကြည့်


<img src="http://drive.google.com/uc?export=view&id=14XzKUefUaeG_Cz0b1E2vsl9RP0ExyEy_" alt="Neural Network architecture for XOR function">
*Neural Network architecture for XOR function*

အဲ့တော့ ဒီမှာ Neuron 1 နဲ့ 2ရှိမယ်နော် အဲ့တော့ အရင်နေ့ကပြောခဲ့တဲ့အတိုင်း Neuron ထဲကို input တစ်ခုဝင်ရင် ဘာ eq နဲ့တွက်ရမလဲ မှတ်မိသေးလား ဇော်ဇော် ပြီးတော့ ဒီneuron နှစ်ခုအတွက်ဆို Weight ဘယ်နခုရှိမလဲ ပြောကြည့်ပါဦး''

"ဟုတ် summation = WX + b ဟုတ်တယ်မလား။ ပြီးတော့ Weight ကဘယ်နခုရှိမလဲဆိုတော့ နှစ်လုံးလားကိုကျော် အရင်နေ့က ကိုကျော်ပြာခဲ့တာက input အရေအတွက်လိုက်ရ်ှိမယ်ဆိုတော့''

"အဲ မဟုတ်ဘူး ငါ့ညီ မင်းက eq တော့မှတ်မိတယ်တော်တယ် ဒါပေမယ့် Weight မှာက ညီနဲနဲထပ်မှတ်ရမယ်။ ဟိုနေ့က vector တွေ matrix တွေပါမယ်လို့ပြောတာမှတ်မိသေးလား''

"ဟုတ်ကိုကျော်''

"ဒါဆို ဒီမှာပြောမယ် ကိုကျော်တို့က input vector တွေကလေ column vector အနေနဲ့ဝင်လာမှာ ဒီတော့ ပထမ ဝင်လာမယ့် x1 လေးဆိုရင် 

x1 = [0<br >
      0]

ဒါမျိုးဝင်လာမယ် ပြီးရင် weight ကျတော့ row vector နဲ့ရှိနေမှာ ဒါမျိုး

w1 = [1.5 0.5]

ဆိုပြီးရှိမယ် ဒီ weight တန်ဖိုးတွေက ဘယ်ကရလဲမှတ်မိသေးလား''

"ဟုတ်ကိုကျော် random ခေါက်ပြီးရတာ''

"Ok ဟုတ်ပြီ ဒါဆို weight ကိုဆက်မယ် ခုနက ကိုကျော်ပြောခဲ့တယ်လေ နဲနဲထပ်မှတ်ရမယ်ဆိုတာ။ ဒီမှာမှတ်မယ် ဟိုနေ့က သင်ပေးတာက Neuron တစ်လုံးပဲရှိလို့ weight က input အရေအတွကအရ ယူရတာ ဒီမှာဆို neuron နှစ်လုံး ''


<img src="http://drive.google.com/uc?export=view&id=14XzKUefUaeG_Cz0b1E2vsl9RP0ExyEy_" alt="Neural Network architecture for XOR function">
*Neural Network architecture for XOR function*

"အဲ့တော့ weight အရေအတွက်သာမက neuron အရေအတွက် အတွက်ပါစဥ်းစားရတော့မယ် ပုံကိုကြည့် x1 က neuron 1 ကိုလဲဝင်မယ် 2 ကိုလဲ ဝင်မယ် ဒါဆို ကြည့် w နောက်မှာ ဂဏန်းနှစ်လုံးပါလာတော့မယ်

w11 နဲ့ w21 ဆိုပြီး တွေ့လိမ့်မယ် ဒါဆိုရင် ရှေ့ကဂဏန်းတစ်လုံးက neuron ရဲ့ နံပါတ် ဒုတိယတစ်လုံးက input ရဲ့ နံပါတ် ဒီတော့ x1 က neuron နှစ်လုံးကိုဝင်မှာဖြစ်တဲ့အတွက် x1 ရယ် neuron1 ရယ်တဲ့ သက်ဆိုင်တဲ့ w ကလဲနှစ်လုံးရှိမယ် w11 နဲ့ w21 ဒီတော့ဆက်ကြည့်မယ် input နံပါတ် 2, x 2အတွက်လဲတူတူပဲ သူကလဲ neuron 2လုံးစလုံးကိုဝင်မှာဖြစ်တဲ့အတွက် သူ့မှာလဲ w12 နဲ့ w22 ဆိုပြီးရှိလာလိမ့်မယ်။ ဒီတော့ weight တွေကို ကြေငြာလိုက်မယ် w11 = 1.5 w12 = 1 w21 = 0.5 w22 = 0.1 ရတယ် စသည်ဖြင့်ဆိုကြပါတော့''

"ဒါဆို bias ကရော ကိုကျော်''

"အဲ bias ကသိပ်စဥ်းစားစရာမလိုဘူး Neuron အရေအတွက်အလိုက် bias ရမယ်''

<img src="http://drive.google.com/uc?export=view&id=14XzKUefUaeG_Cz0b1E2vsl9RP0ExyEy_" alt="Neural Network architecture for XOR function">
*Neural Network architecture for XOR function*

"ဒီပုံကိုကြည့် အလွှာလေးတွေတွေ့တယ်မလား ဘယ်နှလွှာ ရှိတယ်ထင်လဲ''

"သုံးလွှာလား ကိုကျော်''

""မဟုတ်ဘူးကွ ဒါကိုဆို နှစ်လွှာလို့ပဲခေါ်တယ် input ကိုအလွှာလို့မသတ်မှတ်ဘူး hidden layer နဲ့ output layer ကိုပဲအလွှာလို့သတ်မှတ်တာ ဒီတော့ hidden အလွှာမှာဆိုရင် neuron နှစ်လုံးရှိတဲ့အတွက် bias ကလဲ နှစ်လုံးပဲရှိမယ် b1 = 1.2 ဆိုပြီးရတယ်လို့ မှတ်လိုက််မယ် b2 = 0.2 ဆိုပြီး ရလာတယ်ဆိုပါစို့''

"ဒီတော့ ကိုကျော်ပြောမယ် အခုစာအုပ်ယူပြီ ကိုကျော်ပြောတဲ့အတိုင်းချတွက်လိုက်။ ကဲစမယ် အခုဆိုကိုကျော်တို့ summation လို့မရေးပဲ zလို့တည်လိုက်မယ် ဒါဆိုပထမ neuron အတွက်ဘယ်လိုတွက်ရမလဲဆိုတော့ 

z1 = [w11 w12] * [x1 + b1 <br >
x2 ]<br >

z1 = [1.5  1 ] * [0 + 1.2 <br >
0]<br >

z1 = 1.2

ရမယ် ဟုတ်လား။ ဒါဆို summation ရပြီရင် ဘာဆက်တွက်ရမလဲ''

"Activation function ထဲသွားရမယ် ဟုတ်လား ကိုကျော်''

"အံမယ် ဘယ်ဆိုးလို့တုန်းဟ မှတ်မိသားပဲ။ ကဲ ဒါဆို ဒီမှာ sigmoid function လေးကိုသုံးပြီတွက်ပြမယ် 
အဲ့တာဆို sigmoid function လေးကဘယ်လိုလုပ်သလဲဆို သူ့ထဲဝင်ဝင်လာတဲ့နံပါတ်က 0 ဆို 0.5, 0 ထက်ကြီးရင်ကြီးသလောက်  1 ဘက်ခြမ်းကိုကပ်မယ် , 0 ထက်ငယ်ရင် ငယ်သလောက် 0 ဘက်ခြမ်းကပ်မယ် ဆိုပြီးထုတ်ပေးမယ်အဲ့တော့ အောက်က ပုံအတိုင်းပေါ့ကွာ

<img src="http://drive.google.com/uc?export=view&id=1_KoicEsmMoJGk52HObcvUb44hSHJmQ5O" alt="Sigmoid function">
*Sigmoid function*

ဒီတော့

a1 = f(z1)<br >
a1 = f(1.2)<br >
a1 ≈ 1 ( 1 ပဲဆိုကြပါစို့ကွာ)<br >

ရမယ် ပေါ့ ဒီတော့ ဒီ a1 ဆိုတာက neuron1 ထဲမှာ တွက်ချက်မှုတွေလုပ်ပြီးတော့ရလာတဲ့ တန်ဖိုးလေးဖြစ်သွားပြီ။ ကဲဒါဆို ခန ထားထားလိုက်သူ့ကို။ နဲနဲရှုပ်နေလား ဒါဆို ခနနားပြီး နောက်တစ်ခါသေသေချာချာလေးပြန်ဖတ်ကြည့် လက်ကပါချရေး''

"ဟုတ်ကိုကျော် နားတော့နားလည်ပါသေးတယ် ဟိုနေ့ကနဲ့ သိပ်မှမကွာသေးတာ''

"ဟုတ်ပြီ ဒါဆို neuron 2 ကနေ activation တန်ဖိုးလေးတွက်တာကို ကြည့်ကြမယ်

z2 = [w21 w22] * [x1 + b2<br >
x2 ]<br >
z2 = [0.5 0.1 ] * [0 + 0.2<br >
0]<br >
z2 = 0.2

ပြီးတော့ activation function

a2 = f(z2)<br >
a2 = f(0.2)<br >
a2 ≈ 1<br >

အဲ့တော့ ခု a1 နဲ့ a2 ရလာပြီဆိုတော့ အကိုတို့ ပုံထဲမှာပါသလို နောက် တစ်လွှာဖြစ်တဲ့ output layer ကိုသွားဖို့အဆင်သင့်ဖြစ်ပြီပုံကိုကြည့်''

<img src="http://drive.google.com/uc?export=view&id=14XzKUefUaeG_Cz0b1E2vsl9RP0ExyEy_" alt="Neural Network architecture for XOR function">
*Neural Network architecture for XOR function*
"ဒီမှာ output အလွှာမှာ neuron တစ်လုံးပဲရှိတော့တဲ့အတွက် Single layer preceptron နဲ့ ပြန်တူသွားပြီ အဲ့ တော့ weight ဘယ်နခု bias ဘယ်နခုရှိမယ်ထင်လဲ ညီ''

"Weight နှစ်ခုနဲ့ bias တစ်ခုပေါ့ ဟုတ်လား''

"ဟုတ်ပကွာ ဒီတော့ကြည့်ရအောင် w31 w32 နဲ့ b3ပေ့ါ ဒါဆို ကြေငြာလိုက်မယ် w31 = 1.0 w32 = 0.1 b3 = 0.5''

"ဟုတ်ကိုကျော် ဒါဆို input ကဘယ်ကရမလဲ''

"လွယ်ပါတယ်ကွာ ခုနက ရှေ့ အလွှာကရလာတဲ့ a1 နဲ့ a2 က input ပေါ့ကွ ဒါဆိုဘယ်လိုဖြစ်မလဲကြည့်လိုက်

z3 = [w31 w32] * [a1 + b2<br >
a2 ]<br >
z3 = [1.0 0.1 ] * [1 + 0.5<br >
1]<br >
z3 = 1.6

ဒီတော့ activation Function သွားမယ် 

a3 = f(z3)<br >
a3 = f(1.6)<br >
a3 = 1<br >

လို့ပဲယူလိုက်မယ်ဒီတော့ ကိုကျော်တို့လိုချင်တဲ့အဖြေက 0 ဒါပမယ့် ရလာတာက 1 ဒီတော့ မတူတေ့ာဘူး မတူရင် ဘယ်လိုလုပ်ရမလဲ''

"Error တွက်ရမယ် equation က 

Error = target output - prediction output''

"ဟာဟုတ်လချည်လား ဒီတော့တွက်ကြည့်ရအောင်

Error = 0 - 1 <br >
Error = -1 <br >

ရတယ်ပေါ့ ဒါပေါမယ် Multi layer မှာကျ weight update လုပ်တဲ့ခါ error ကို ဒီလိုမတွက်ဘူး ဘယ်လို့လဲဆိုတော့ a3 ထဲမှာ w12 တို့ w21 တို့စတဲ့ weight တွေက တိုက်ရိုက်မရှိဘူး အဲ့တော့ဒီလိုတွက်ရမယ် ပထမဆုံး eq တွေအရင်ပြောသွားလိုက်မယ် ဒါနဲ့ weight update လုပ်ရင် ဘာလိုဦးမလဲ''

"Learning rate လားကိုကျော်''

"အင်း ဟုတ်တယ် ဒါဆို learning rate ကို 0.1လို့ပေးလိုက်မယ်

b1(new) = b1(old) + learning rate * (target output - z1) <br >
b2(new) = b2(old) + learning rate * (target output - z2)<br >
b3(new) = b2(old) + learning rate * (target output - z3)<br >
w11(new) = w11(old) + learning rate * (target output - z1)<br >
w12(new) = w12(old) + learning rate * (target output - z1)<br >
w21(new) = w21(old) + learning rate * (target output - z2)<br >
w22(new) = w22(old) + learning rate * (target output - z2)<br >
w31(new) = w31(old) + learning rate * (target output - z3)<br >
w32(new) = w32(old) + learning rate * (target output - z3)<br >
"
"ကိုကျော် ဘာလို့အရင်လို error ကိုမတွက်တော့တာလဲ''

"အဲ အခုဒါကို ဒီလိုပဲခန မှတ်ထား အခု w11 ကို update လုပ်မယ်ဆိုရင် z1 နဲ့ဘာလို့ယူသလဲ z3နဲ့ ဘာလို့မယူလဲဆိုတော့ w11 က z3ထဲမှာတိုက်ရိုက်မရှိပဲ တိုက်ရိုက်ရှိတဲ့ z1 နဲ့တွက်ရတာဖြစ်တယ် ဒီလောက်ပဲမှတ်ထား နောက်Back Propagation ကိုသေချာလေးရှင်းပြတဲ့ခါကျမှ နားလည်သွားလိမ့်မယ် ''

ဇော်ဇော့်မှာ ကြောင်တောင်တောင် မျက်လုံးကလေးပေကလပ် ပေကလပ်ဖြင့် 

"ဟုတ် နဲနဲတော့ရှုပ်နေသေးတယ် ဒါပေမယ့်ရေးမှတ်ထားလိုက်မယ် ခုနက ကိုကျော်ပြောတာဘာ back ဘာဆိုလား အဲ့တာနဲ့မှတွဲပြီးစဥ်းစားယူမယ်ရတယ်မလား''

"Back Propagation ပါကွ ရတယ် ညီ အဆင်ပြေသွားလိမ့်မယ်မပူနဲ့ ဒီတော့ချတွက်ကြမယ်

b1(new) = 1.2 + 0.1 (0 - 1.2) =1.08 <br >
b2(new) = 0.2 + 0.1 (0 - 0.2) = 0.18<br >
b3(new) = 0.5 + 0.1 (0 - 1.6) = 0.34<br >
w11(new) = 1.5 + 0.1 * (0 - 1.2) = 1.38<br >
w12(new) = 1 + 0.1 * (0 - 1.2) = 0.88<br >
w21(new) = 0.5 + 0.1 (0 - 0.2) = 0.48<br >
w22(new) = 0.1 + 0.1 (0 - 0.2) = 0.08<br >
w31(new) = 1 + 0.1 (0 - 1.6) = 0.84<br >
w32(new) = 0.1 + 0.1 (0 - 1.6) = -0.06<br >

စသည်ဖြင့် အသစ်တွေရလာမယ် ဒါဆို ဒီ weight အသစ်တွေနဲ့ အစကနေပြန်ပြီး တစ်ဖန်ပြန်တွက်ရမယ် Weight တွေ Bias တွေကိုတန်ဖိုး အမှန်ရတဲ့ထိ တစ်နည်းအားဖြင့် Error = 0 မရောက်မချင်းပေါ့ နားလည်လားဇော်ဇော် 

နားမလည်သေးရင်ဖြည်းဖြည်းချင်းစဥ်းစား အများကြီးမစဥ်းစားနဲ့ဟုတ်ပြီလား''

"ဟုတ်ကိုကျော်ရေ သေချာလေးစဥ်းစားပြီး တစ်ဆင့်ချင်းပြန်လုပ်ကြည့်ဦးမယ် ''

"အေးကွာ... ဟုတ်ပြီ ဒါဆိုလဲ ဒီလောက်ပဲ နောက်နေ့ကျမှ Forward Propagation Backward Propagation ဒါတွေလာမယ် ခုတော့ နားတော့ ပြီးမှစဥ်းစား''
"ဟုတ်... ကိုကျော်''

References:
1. [Neural networks and Deep learning - Andrew Ng](https://www.coursera.org/learn/neural-networks-deep-learning){:target="_blank"}
2. Artificial Neural Network အခြေခံသီအိုရီနှင့်အသုံးပြုမှု နည်းလမ်းများ - ဆရာဇော်မင်းခိုင်
3. Photo source: Internet
